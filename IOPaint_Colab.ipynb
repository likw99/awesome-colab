{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likw99/awesome_colab/blob/main/IOPaint_Colab.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "s5KZpHk-4GPO",
        "outputId": "650ed304-2cb6-484a-fce1-753d92693e9a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "lida 0.0.10 requires kaleido, which is not installed.\n",
            "llmx 0.0.15a0 requires cohere, which is not installed.\n",
            "llmx 0.0.15a0 requires openai, which is not installed.\n",
            "llmx 0.0.15a0 requires tiktoken, which is not installed.\n",
            "tensorflow-probability 0.22.0 requires typing-extensions<4.6.0, but you have typing-extensions 4.9.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mRemember, installing models for the first time will be very slow! Be patient.\n",
            "IOPaint public URL: NgrokTunnel: \"https://1f22-34-125-27-170.ngrok-free.app\" -> \"http://localhost:8000\"\n",
            "Please wait until the \"Application startup complete.\" message appears...\n",
            ">>> starting iopaint start --model lama --device cuda --port 8000\n",
            "2024-02-06 05:45:58.899 | INFO     | iopaint.runtime:setup_model_dir:82 - Model directory: /root/.cache\n",
            "- Platform: Linux-6.1.58+-x86_64-with-glibc2.35\n",
            "- Python version: 3.10.12\n",
            "- torch: 2.1.0+cu121\n",
            "- torchvision: 0.16.0+cu121\n",
            "- Pillow: 9.5.0\n",
            "- diffusers: 0.26.1\n",
            "- transformers: 4.35.2\n",
            "- opencv-python: 4.8.0.76\n",
            "- accelerate: 0.26.1\n",
            "- iopaint: 1.0.3\n",
            "- rembg: N/A\n",
            "- realesrgan: N/A\n",
            "- gfpgan: N/A\n",
            "\n",
            "The cache for model files in Transformers v4.22.0 has been updated. Migrating your old cache. This is a one-time only operation. You can interrupt this and resume the migration later on by calling `transformers.utils.move_cache()`.\n",
            "0it [00:00, ?it/s]\n",
            "2024-02-06 05:46:04.648 | INFO     | iopaint.cli:start:161 - lama not found in /root/.cache, try to downloading\n",
            "2024-02-06 05:46:04.648 | INFO     | iopaint.download:cli_download_model:26 - Downloading lama...\n",
            "Downloading: \"https://github.com/Sanster/models/releases/download/add_big_lama/big-lama.pt\" to /root/.cache/torch/hub/checkpoints/big-lama.pt\n",
            "100%|██████████| 196M/196M [00:00<00:00, 284MB/s]\n",
            "2024-02-06 05:46:06.266 | INFO     | iopaint.helper:download_model:54 - Download model success, md5: e3aa4aaa15225a33ec84f9f4bc47e500\n",
            "2024-02-06 05:46:06.266 | INFO     | iopaint.download:cli_download_model:28 - Done.\n",
            "2024-02-06 05:46:06.573 | INFO     | iopaint.model_manager:init_model:39 - Loading model: lama\n",
            "2024-02-06 05:46:06.573 | INFO     | iopaint.helper:load_jit_model:104 - Loading model from: /root/.cache/torch/hub/checkpoints/big-lama.pt\n",
            "INFO:     Started server process [2378]\n",
            "INFO:     Waiting for application startup.\n",
            "INFO:     Application startup complete.\n",
            "INFO:     Uvicorn running on http://127.0.0.1:8000 (Press CTRL+C to quit)\n",
            "{\n",
            "\"host\": \"127.0.0.1\",\n",
            "\"port\": 8000,\n",
            "\"model\": \"lama\",\n",
            "\"no_half\": false,\n",
            "\"low_mem\": false,\n",
            "\"cpu_offload\": false,\n",
            "\"disable_nsfw_checker\": false,\n",
            "\"local_files_only\": false,\n",
            "\"cpu_textencoder\": false,\n",
            "\"device\": \"cuda\",\n",
            "\"input\": null,\n",
            "\"output_dir\": null,\n",
            "\"quality\": 95,\n",
            "\"enable_interactive_seg\": false,\n",
            "\"interactive_seg_model\": \"vit_b\",\n",
            "\"interactive_seg_device\": \"cpu\",\n",
            "\"enable_remove_bg\": false,\n",
            "\"enable_anime_seg\": false,\n",
            "\"enable_realesrgan\": false,\n",
            "\"realesrgan_device\": \"cpu\",\n",
            "\"realesrgan_model\": \"realesr-general-x4v3\",\n",
            "\"enable_gfpgan\": false,\n",
            "\"gfpgan_device\": \"cpu\",\n",
            "\"enable_restoreformer\": false,\n",
            "\"restoreformer_device\": \"cpu\"\n",
            "}\n",
            "INFO:     202.78.172.69:0 - \"GET / HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /assets/index-NQyCh9rO.js HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /assets/index-iK_rIxV0.css HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /socket.io/?EIO=4&transport=polling&t=Oryz26y HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /api/v1/inputimage HTTP/1.1\" 404 Not Found\n",
            "INFO:     202.78.172.69:0 - \"GET /api/v1/models HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /api/v1/model HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /api/v1/server-config HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /favicon.ico HTTP/1.1\" 404 Not Found\n",
            "INFO:     202.78.172.69:0 - \"POST /socket.io/?EIO=4&transport=polling&t=Oryz2CE&sid=bH3gGEugQx2XWuQvAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /socket.io/?EIO=4&transport=polling&t=Oryz2CG&sid=bH3gGEugQx2XWuQvAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     ('202.78.172.69', 0) - \"WebSocket /socket.io/?EIO=4&transport=websocket&sid=bH3gGEugQx2XWuQvAAAA\" [accepted]\n",
            "INFO:     connection open\n",
            "INFO:     202.78.172.69:0 - \"GET /socket.io/?EIO=4&transport=polling&t=Oryz2HZ&sid=bH3gGEugQx2XWuQvAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"GET /socket.io/?EIO=4&transport=polling&t=Oryz2Xe&sid=bH3gGEugQx2XWuQvAAAA HTTP/1.1\" 200 OK\n",
            "INFO:     202.78.172.69:0 - \"POST /api/v1/gen-info HTTP/1.1\" 200 OK\n",
            "2024-02-06 05:48:17.414 | INFO     | iopaint.model.base:__call__:99 - Run crop strategy\n",
            "2024-02-06 05:48:19.824 | INFO     | iopaint.api:api_inpaint:241 - process time: 2410.05ms\n",
            "INFO:     202.78.172.69:0 - \"POST /api/v1/inpaint HTTP/1.1\" 200 OK\n",
            "2024-02-06 05:48:28.456 | INFO     | iopaint.model.base:__call__:99 - Run crop strategy\n",
            "2024-02-06 05:48:29.467 | INFO     | iopaint.api:api_inpaint:241 - process time: 1011.77ms\n",
            "INFO:     202.78.172.69:0 - \"POST /api/v1/inpaint HTTP/1.1\" 200 OK\n",
            "2024-02-06 05:48:40.596 | INFO     | iopaint.model.base:__call__:99 - Run crop strategy\n",
            "2024-02-06 05:48:41.215 | INFO     | iopaint.api:api_inpaint:241 - process time: 619.12ms\n",
            "INFO:     202.78.172.69:0 - \"POST /api/v1/inpaint HTTP/1.1\" 200 OK\n",
            "2024-02-06 05:48:51.338 | INFO     | iopaint.model.base:__call__:99 - Run crop strategy\n",
            "2024-02-06 05:48:51.986 | INFO     | iopaint.api:api_inpaint:241 - process time: 648.21ms\n",
            "INFO:     202.78.172.69:0 - \"POST /api/v1/inpaint HTTP/1.1\" 200 OK\n"
          ]
        }
      ],
      "source": [
        "#@title IOPaint in Google Colab\n",
        "#@markdown In order to use this colab:\n",
        "#@markdown - Enter your [ngrok auth token](https://dashboard.ngrok.com/get-started/your-authtoken)\n",
        "#@markdown - Select a model ([find documentation here](https://www.iopaint.com/models))\n",
        "#@markdown - Select the T4 GPU accelerator at `Runtime > Change runtime type > Hardware accelerator`\n",
        "#@markdown - Click `Runtime > Run all`\n",
        "#@markdown - Follow the link to access the UI\n",
        "\n",
        "#@markdown Note: beware that installing models for the first time will take some time.\n",
        "ngrok_token = \"27wkTJk6QE901Le8D1b2xQOMzOX_3UBxQxEb3n5yBNi4bs7CQ\" # @param {type:\"string\"}\n",
        "model = \"lama\" # @param [\"lama\", \"mat\", \"migan\", \"ldm\", \"zits\", \"fcf\", \"manga\", \"Sanster/PowerPaint-V1-stable-diffusion-inpainting\", \"Sanster/AnyText\", \"timbrooks/instruct-pix2pix\", \"Fantasy-Studio/Paint-by-Example\", \"kandinsky-community/kandinsky-2-2-decoder-inpaint\"]\n",
        "\n",
        "# Download and install ollama to the system\n",
        "!pip install iopaint pyngrok > install_logs.txt\n",
        "\n",
        "import os\n",
        "import asyncio\n",
        "from pyngrok import ngrok\n",
        "import sys\n",
        "\n",
        "os.environ.update({'LD_LIBRARY_PATH': '/usr/lib64-nvidia'})\n",
        "\n",
        "print(\"Remember, installing models for the first time will be very slow! Be patient.\")\n",
        "\n",
        "async def run_process(cmd):\n",
        "  print('>>> starting', *cmd)\n",
        "  p = await asyncio.subprocess.create_subprocess_exec(\n",
        "      *cmd,\n",
        "      stdout=asyncio.subprocess.PIPE,\n",
        "      stderr=asyncio.subprocess.PIPE,\n",
        "  )\n",
        "\n",
        "  async def pipe(lines):\n",
        "    async for line in lines:\n",
        "      print(line.strip().decode('utf-8'))\n",
        "\n",
        "  await asyncio.gather(\n",
        "      pipe(p.stdout),\n",
        "      pipe(p.stderr),\n",
        "  )\n",
        "\n",
        "if not ngrok_token:\n",
        "  print(\"Hey! You haven't set the Ngrok token. Without the token, the app won't start. Please paste your token in the input.\")\n",
        "  sys.exit(\"Ngrok token not set\")\n",
        "\n",
        "ngrok.set_auth_token(ngrok_token)\n",
        "active_tunnels = ngrok.get_tunnels()\n",
        "for tunnel in active_tunnels:\n",
        "    public_url = tunnel.public_url\n",
        "    ngrok.disconnect(public_url)\n",
        "url = ngrok.connect(addr=\"8000\", bind_tls=True)\n",
        "print(f\"IOPaint public URL: {url}\")\n",
        "print(\"Please wait until the \\\"Application startup complete.\\\" message appears...\")\n",
        "\n",
        "await asyncio.gather(\n",
        "    run_process(['iopaint', 'start', '--model', model, '--device', 'cuda', '--port', '8000']),\n",
        ")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}