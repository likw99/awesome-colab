{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "A100",
      "collapsed_sections": [
        "dDxRMA_BCBAt"
      ],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likw99/awesome-colab/blob/main/warren_buffett_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fine-tuning Mistral-7b-Instruct to become Warren Buffett"
      ],
      "metadata": {
        "id": "-7hYqUsxYQtl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### imports"
      ],
      "metadata": {
        "id": "NT-8d3vZYG4W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install auto-gptq\n",
        "!pip install optimum\n",
        "!pip install bitsandbytes"
      ],
      "metadata": {
        "id": "XeaOnjP2zsy-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# resolving \"No inf checks were recorded for this optimizer.\" issue\n",
        "!pip uninstall torch -y\n",
        "!pip install torch==2.1"
      ],
      "metadata": {
        "id": "ik7WwhkFftE0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
        "from peft import prepare_model_for_kbit_training\n",
        "from peft import LoraConfig, get_peft_model\n",
        "from datasets import load_dataset\n",
        "import transformers"
      ],
      "metadata": {
        "id": "tfSl5xRs0y8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load model"
      ],
      "metadata": {
        "id": "VVstnrh-0m2x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "model = AutoModelForCausalLM.from_pretrained(\n",
        "    model_name,\n",
        "    device_map=\"auto\", # automatically figures out how to best use CPU + GPU for loading model\n",
        "    trust_remote_code=False, # prevents running custom model files on your machine\n",
        "    revision=\"main\", # which version of model to use in repo\n",
        ")"
      ],
      "metadata": {
        "id": "GFcKal6c96su"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load tokenizer"
      ],
      "metadata": {
        "id": "bCKC3_yl0pNS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "c3CIa8C80Vtm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Prepare Model for Training"
      ],
      "metadata": {
        "id": "nXc8pBsVIy2N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.train() # model in training mode (dropout modules are activated)\n",
        "\n",
        "# enable gradient check pointing\n",
        "model.gradient_checkpointing_enable()\n",
        "\n",
        "# enable quantized training\n",
        "model = prepare_model_for_kbit_training(model)"
      ],
      "metadata": {
        "id": "K3PesDIyI1Hn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# LoRA config\n",
        "config = LoraConfig(\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    target_modules=[\"q_proj\"],\n",
        "    lora_dropout=0.05,\n",
        "    bias=\"none\",\n",
        "    task_type=\"CAUSAL_LM\"\n",
        ")\n",
        "\n",
        "# LoRA trainable version of model\n",
        "model = get_peft_model(model, config)\n",
        "\n",
        "# trainable parameter count\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "iXOHXZcII5FS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Preparing Training Dataset"
      ],
      "metadata": {
        "id": "tssCKXx3hUqM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# load dataset\n",
        "data = load_dataset(\"eagle0504/warren-buffett-annual-letters-from-1977-to-2019\")"
      ],
      "metadata": {
        "id": "MWcyH5qG5ErX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import DatasetDict\n",
        "\n",
        "def has_single_pair(example):\n",
        "    text = example['text']\n",
        "    return text.count('### Human') == 1 and text.count('### Assistant') == 1\n",
        "\n",
        "# Filter the dataset to include only rows with a single question + answer pair\n",
        "new_dataset = data.filter(has_single_pair)\n",
        "\n",
        "# Optional: Select a small subset of rows from the 'test' split to (a) prove concept and (b) fine-tune more quickly\n",
        "# new_dataset = DatasetDict({\n",
        "#     'train': new_dataset['train'].select(range(50)),\n",
        "#     'test': new_dataset['test'].select(range(10))\n",
        "# })"
      ],
      "metadata": {
        "id": "RktgRguo5PBl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Transform Dataset so that it follows Mistral-7B Instruct's fine-tuning format"
      ],
      "metadata": {
        "id": "JcZErIrW7KyU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "system_prompt = f\"\"\"BuffettGPT, is a virtual financial assistant that communicates in clear, accessible language, escalating to technical financial depth upon request. \\\n",
        "It reacts to feedback aptly and ends responses with its signature '-BuffettGPT'. \\\n",
        "BuffettGPT will tailor the length of its responses to match the question length, providing concise and accurate responses to financial questions about Berkshire Hathaway, its businesses, and stocks in general, \\\n",
        "thus keeping the interaction natural and engaging.\n",
        "\n",
        "Please answer the following question.\n",
        "\"\"\""
      ],
      "metadata": {
        "id": "kt2-rZU7CHrJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def transform_text(example):\n",
        "    text = example['text']\n",
        "\n",
        "    # Split the text into the question and answer\n",
        "    question, answer = text.split('### Assistant:')\n",
        "\n",
        "    # Transform the question\n",
        "    transformed_question = question.replace('### Human:', '')\n",
        "    transformed_question = transformed_question.strip() + ' [/INST]'\n",
        "\n",
        "    # Transform the answer\n",
        "    transformed_answer = ' ' + answer.strip() + ' -BuffettGPT</s>'\n",
        "\n",
        "    # Combine the system prompt, transformed question, and transformed answer\n",
        "    transformed_text = '<s>[INST] ' + system_prompt + ' ' + transformed_question + transformed_answer\n",
        "\n",
        "    return {'text': transformed_text}\n",
        "\n",
        "\n",
        "# Apply the transformation to the new_dataset\n",
        "transformed_dataset = new_dataset.map(transform_text, remove_columns=new_dataset['train'].column_names)"
      ],
      "metadata": {
        "id": "mne8ZWaO-Tki"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# create tokenize function\n",
        "def tokenize_function(examples):\n",
        "    # extract text\n",
        "    text = examples[\"text\"]\n",
        "\n",
        "    #tokenize and truncate text\n",
        "    tokenizer.truncation_side = \"left\"\n",
        "    tokenized_inputs = tokenizer(\n",
        "        text,\n",
        "        return_tensors=\"np\",\n",
        "        truncation=True,\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    return tokenized_inputs\n",
        "\n",
        "# tokenize training and validation datasets\n",
        "tokenized_data = transformed_dataset.map(tokenize_function, batched=True)"
      ],
      "metadata": {
        "id": "D9LEA_g9gZPF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# setting pad token\n",
        "tokenizer.pad_token = tokenizer.eos_token\n",
        "# data collator\n",
        "data_collator = transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)"
      ],
      "metadata": {
        "id": "pow8yLjKcJM8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Fine-tuning Model"
      ],
      "metadata": {
        "id": "e-evbaTxhQTC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparameters\n",
        "lr = 2e-4\n",
        "batch_size = 4\n",
        "num_epochs = 10\n",
        "\n",
        "# define training arguments\n",
        "training_args = transformers.TrainingArguments(\n",
        "    output_dir= \"buffettgpt-ft\",\n",
        "    learning_rate=lr,\n",
        "    per_device_train_batch_size=batch_size,\n",
        "    per_device_eval_batch_size=batch_size,\n",
        "    num_train_epochs=num_epochs,\n",
        "    weight_decay=0.01,\n",
        "    logging_strategy=\"epoch\",\n",
        "    evaluation_strategy=\"epoch\",\n",
        "    save_strategy=\"epoch\",\n",
        "    load_best_model_at_end=True,\n",
        "    gradient_accumulation_steps=4,\n",
        "    warmup_steps=2,\n",
        "    fp16=True,\n",
        "    optim=\"paged_adamw_8bit\",\n",
        "\n",
        ")"
      ],
      "metadata": {
        "id": "e1PPLr4McNii"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# configure trainer\n",
        "trainer = transformers.Trainer(\n",
        "    model=model,\n",
        "    train_dataset=tokenized_data[\"train\"],\n",
        "    eval_dataset=tokenized_data[\"test\"],\n",
        "    args=training_args,\n",
        "    data_collator=data_collator\n",
        ")\n",
        "\n",
        "\n",
        "# train model\n",
        "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!\n",
        "trainer.train()\n",
        "\n",
        "# renable warnings\n",
        "model.config.use_cache = True"
      ],
      "metadata": {
        "id": "m6wZK62bJKsJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Push model to hub"
      ],
      "metadata": {
        "id": "e3-3yynACoX0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from huggingface_hub import notebook_login\n",
        "notebook_login()\n",
        "\n",
        "# # option 2: key login\n",
        "# from huggingface_hub import login\n",
        "# write_key = 'hf_' # paste token here\n",
        "# login(write_key)"
      ],
      "metadata": {
        "id": "_c5KhuhpCno-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "hf_name = 'virattt' # your hf username or org name\n",
        "model_id = hf_name + \"/\" + \"buffettgpt-ft\""
      ],
      "metadata": {
        "id": "PYLbvvuPCq7q"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.push_to_hub(model_id)\n",
        "trainer.push_to_hub(model_id)"
      ],
      "metadata": {
        "id": "c4nAFjYPDC4_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load Fine-tuned Model (if you already uploaded it before)"
      ],
      "metadata": {
        "id": "dDxRMA_BCBAt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# # load model from hub\n",
        "# from peft import PeftModel, PeftConfig\n",
        "# from transformers import AutoModelForCausalLM\n",
        "\n",
        "# model_name = \"TheBloke/Mistral-7B-Instruct-v0.2-GPTQ\"\n",
        "# model = AutoModelForCausalLM.from_pretrained(model_name,\n",
        "#                                              device_map=\"auto\",\n",
        "#                                              trust_remote_code=False,\n",
        "#                                              revision=\"main\")\n",
        "\n",
        "# config = PeftConfig.from_pretrained(\"virattt/buffettgpt-ft\")\n",
        "# model = PeftModel.from_pretrained(model, \"virattt/buffettgpt-ft\")\n",
        "\n",
        "# # load tokenizer\n",
        "# tokenizer = AutoTokenizer.from_pretrained(model_name, use_fast=True)"
      ],
      "metadata": {
        "id": "T9gRgNjNiTvK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Use Fine-tuned Model"
      ],
      "metadata": {
        "id": "UJIo4nUnhgiU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prompt_template = lambda question: f'''[INST] {system_prompt} \\n{question} \\n[/INST]'''\n",
        "\n",
        "question = \"What was operating earnings a year ago?\"\n",
        "\n",
        "prompt = prompt_template(question)\n",
        "print(prompt)"
      ],
      "metadata": {
        "id": "ITUJZDIYjs4t"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval()\n",
        "\n",
        "inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
        "outputs = model.generate(input_ids=inputs[\"input_ids\"].to(\"cuda\"), max_new_tokens=280)\n",
        "\n",
        "print(tokenizer.batch_decode(outputs)[0])"
      ],
      "metadata": {
        "id": "j9wNZ2URivBW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}