{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/likw99/awesome_colab/blob/main/Lamini_Finetuning_for_Free.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zma3qsYTsVk8"
      },
      "source": [
        "![Logo - Colab.png](data:image/png;base64,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)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4dIzkKjG1f_R"
      },
      "source": [
        "# [Lamini](https://www.lamini.ai/): The LLM engine for rapidly customizing models ü¶ô\n",
        "Walk through Lamini's finetuning pipeline, so you can train custom models on your data.\n",
        "\n",
        "- It's free at $0 per training run.\n",
        "- It's fast at less than 15 minutes.\n",
        "- It's similar to a nearly unlimited prompt size. The toy example here takes in ~120k tokens, more than the largest prompt sizes.\n",
        "- It's learning new information, not just trying to make sense of it given what it already learned (retrieval-augmented generation).\n",
        "\n",
        "\n",
        "This walkthrough goes through a basic question-answer LLM over your data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MJqbS0TYd7N5"
      },
      "source": [
        "# Setup üõ†Ô∏è"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IZ432VBKOMGV",
        "outputId": "73326556-cb72-4236-87fa-7be0e15999d0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<Response [200]>\n"
          ]
        }
      ],
      "source": [
        "# @title Step 1: Authenticate with Google\n",
        "# @markdown Note: You will be asked to sign in with Google, connected to your Lamini account.\n",
        "\n",
        "from google.colab import auth\n",
        "import requests\n",
        "import os\n",
        "import yaml\n",
        "\n",
        "def authenticate_powerml():\n",
        "  auth.authenticate_user()\n",
        "  gcloud_token = !gcloud auth print-access-token\n",
        "  powerml_token_response = requests.get('https://api.powerml.co/v1/auth/verify_gcloud_token?token=' + gcloud_token[0])\n",
        "  print(powerml_token_response)\n",
        "  return powerml_token_response.json()['token']\n",
        "\n",
        "key = authenticate_powerml()\n",
        "\n",
        "config = {\n",
        "    \"production\": {\n",
        "        \"key\": key,\n",
        "        \"url\": \"https://api.powerml.co\"\n",
        "    }\n",
        "}\n",
        "\n",
        "keys_dir_path = '/root/.powerml'\n",
        "os.makedirs(keys_dir_path, exist_ok=True)\n",
        "\n",
        "keys_file_path = keys_dir_path + '/configure_llama.yaml'\n",
        "with open(keys_file_path, 'w') as f:\n",
        "  yaml.dump(config, f, default_flow_style=False)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "lMkskdHX_W0n",
        "outputId": "14dc0041-563e-4647-ad5b-9ef1fa9c0228"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting lamini==0.0.19\n",
            "  Downloading lamini-0.0.19-19-py3-none-any.whl (32 kB)\n",
            "Collecting pydantic==1.10.* (from lamini==0.0.19)\n",
            "  Downloading pydantic-1.10.11-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m34.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-configuration[yaml] (from lamini==0.0.19)\n",
            "  Downloading python_configuration-0.8.2-py3-none-any.whl (22 kB)\n",
            "Collecting requests (from lamini==0.0.19)\n",
            "  Downloading requests-2.31.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m62.6/62.6 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers (from lamini==0.0.19)\n",
            "  Downloading tokenizers-0.13.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.8/7.8 MB\u001b[0m \u001b[31m92.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tqdm (from lamini==0.0.19)\n",
            "  Downloading tqdm-4.65.0-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy (from lamini==0.0.19)\n",
            "  Downloading numpy-1.25.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m79.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-learn (from lamini==0.0.19)\n",
            "  Downloading scikit_learn-1.3.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m97.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting jsonlines (from lamini==0.0.19)\n",
            "  Downloading jsonlines-3.1.0-py3-none-any.whl (8.6 kB)\n",
            "Collecting pandas (from lamini==0.0.19)\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m91.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing-extensions>=4.2.0 (from pydantic==1.10.*->lamini==0.0.19)\n",
            "  Downloading typing_extensions-4.7.1-py3-none-any.whl (33 kB)\n",
            "Collecting attrs>=19.2.0 (from jsonlines->lamini==0.0.19)\n",
            "  Downloading attrs-23.1.0-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.2/61.2 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dateutil>=2.8.2 (from pandas->lamini==0.0.19)\n",
            "  Downloading python_dateutil-2.8.2-py2.py3-none-any.whl (247 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m247.7/247.7 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pytz>=2020.1 (from pandas->lamini==0.0.19)\n",
            "  Downloading pytz-2023.3-py2.py3-none-any.whl (502 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m502.3/502.3 kB\u001b[0m \u001b[31m38.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tzdata>=2022.1 (from pandas->lamini==0.0.19)\n",
            "  Downloading tzdata-2023.3-py2.py3-none-any.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m341.8/341.8 kB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pyyaml<6.0,>=5.1 (from python-configuration[yaml]->lamini==0.0.19)\n",
            "  Downloading PyYAML-5.4.1.tar.gz (175 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m175.1/175.1 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting charset-normalizer<4,>=2 (from requests->lamini==0.0.19)\n",
            "  Downloading charset_normalizer-3.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (201 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m201.8/201.8 kB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting idna<4,>=2.5 (from requests->lamini==0.0.19)\n",
            "  Downloading idna-3.4-py3-none-any.whl (61 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m61.5/61.5 kB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting urllib3<3,>=1.21.1 (from requests->lamini==0.0.19)\n",
            "  Downloading urllib3-2.0.3-py3-none-any.whl (123 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m123.6/123.6 kB\u001b[0m \u001b[31m12.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting certifi>=2017.4.17 (from requests->lamini==0.0.19)\n",
            "  Downloading certifi-2023.5.7-py3-none-any.whl (156 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m157.0/157.0 kB\u001b[0m \u001b[31m15.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scipy>=1.5.0 (from scikit-learn->lamini==0.0.19)\n",
            "  Downloading scipy-1.11.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (36.3 MB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m36.3/36.3 MB\u001b[0m \u001b[31m34.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting joblib>=1.1.1 (from scikit-learn->lamini==0.0.19)\n",
            "  Downloading joblib-1.3.1-py3-none-any.whl (301 kB)\n",
            "\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m302.0/302.0 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting threadpoolctl>=2.0.0 (from scikit-learn->lamini==0.0.19)\n",
            "  Downloading threadpoolctl-3.1.0-py3-none-any.whl (14 kB)\n",
            "Collecting six>=1.5 (from python-dateutil>=2.8.2->pandas->lamini==0.0.19)\n",
            "  Downloading six-1.16.0-py2.py3-none-any.whl (11 kB)\n",
            "Building wheels for collected packages: pyyaml\n",
            "  Building wheel for pyyaml (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyyaml: filename=PyYAML-5.4.1-cp310-cp310-linux_x86_64.whl size=45658 sha256=6be95aa9d36784d7021feb063a573d5e57f951060e8510b6780eb461dc6cbdad\n",
            "  Stored in directory: /root/.cache/pip/wheels/c7/0d/22/696ee92245ad710f506eee79bb05c740d8abccd3ecdb778683\n",
            "Successfully built pyyaml\n",
            "Installing collected packages: tokenizers, pytz, urllib3, tzdata, typing-extensions, tqdm, threadpoolctl, six, pyyaml, python-configuration, numpy, joblib, idna, charset-normalizer, certifi, attrs, scipy, requests, python-dateutil, pydantic, jsonlines, scikit-learn, pandas, lamini\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "google-colab 1.0.0 requires pandas==1.5.3, but you have pandas 2.0.3 which is incompatible.\n",
            "google-colab 1.0.0 requires requests==2.27.1, but you have requests 2.31.0 which is incompatible.\n",
            "numba 0.56.4 requires numpy<1.24,>=1.18, but you have numpy 1.25.1 which is incompatible.\n",
            "tensorflow 2.12.0 requires numpy<1.24,>=1.22, but you have numpy 1.25.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed attrs-23.1.0 certifi-2023.5.7 charset-normalizer-2.0.12 idna-3.4 joblib-1.3.1 jsonlines-3.1.0 lamini-0.0.19 numpy-1.22.4 pandas-1.5.3 pydantic-1.10.11 python-configuration-0.8.2 python-dateutil-2.8.2 pytz-2022.7.1 pyyaml-5.4.1 requests-2.27.1 scikit-learn-1.2.2 scipy-1.10.1 six-1.16.0 threadpoolctl-3.1.0 tokenizers-0.13.3 tqdm-4.65.0 typing-extensions-4.7.1 tzdata-2023.3 urllib3-1.26.16\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "certifi",
                  "charset_normalizer",
                  "dateutil",
                  "numpy",
                  "requests",
                  "six",
                  "urllib3",
                  "yaml"
                ]
              }
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "# @title Step 2: Install the open-source [Lamini library](https://pypi.org/project/lamini/) to use LLMs easily\n",
        "# @markdown Note: After installing, click the \"RESTART RUNTIME\" button at the end of the output, then go onto the next cell.\n",
        "# @markdown Lamini is just on a more recent version of numpy than Colab.\n",
        "!pip install --upgrade --force-reinstall --ignore-installed lamini==0.0.19"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLmpQtZ82TXQ"
      },
      "source": [
        "# Prepare your data üìä\n",
        "\n",
        "Upload your question-answer data in the following format (jsonl):\n",
        "```\n",
        "{\"question\": \"type your question\", \"answer\": \"answer to the question\"}\n",
        "\n",
        "```\n",
        "Upload your question-answer data in the following format (csv):\n",
        "```\n",
        "Make sure that you have 'question' and 'answer' as column keys\n",
        "\n",
        "```\n",
        "You can also download a sample `seed_lamini_docs.jsonl` file, with Lamini question-answer data in it ü¶ô\n",
        "\n",
        "Also we have some more example related to Taylor Swift &nbsp;üëë, BTS &nbsp;üíú, and Open LLMs &nbsp;üìö, try it out!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uNC7eRO5eeXl"
      },
      "outputs": [],
      "source": [
        "!wget -q -O \"seed_lamini_docs.jsonl\" \"https://drive.google.com/uc?export=download&id=1SfGp1tVuLTs0WYDugZcxX-EHrmDtYrYJ\"\n",
        "!wget -q -O \"seed_taylor_swift.jsonl\" \"https://drive.google.com/uc?export=download&id=119sHYYImcXEbGyvS3wWGpkSEVIFdLy6Z\"\n",
        "!wget -q -O \"seed_bts.csv\" \"https://drive.google.com/uc?export=download&id=1lblhdhKwoiOjlvfk8tr7Ieo4KpvjRm6n\"\n",
        "!wget -q -O \"seed_open_llm.jsonl\" \"https://drive.google.com/uc?export=download&id=1S7oPPko-UmOr-bqkZ_PREfGKO2f73ZiK\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UUo9HX0lOw1S"
      },
      "outputs": [],
      "source": [
        "# Functions for printing results during training...\n",
        "def print_training_results(results):\n",
        "    print(\"-\"*100)\n",
        "    print(\"Training Results\")\n",
        "    print(results)\n",
        "    print(\"-\"*100)\n",
        "\n",
        "# ...and after training (inference/runtime)\n",
        "def print_inference(question, finetune_answer, base_answer):\n",
        "    print('Running Inference for: '+ question)\n",
        "    print(\"-\"*100)\n",
        "    print(\"Finetune model answer: \", finetune_answer)\n",
        "    print(\"-\"*100)\n",
        "    print(\"Base model answer: \", base_answer)\n",
        "    print(\"-\"*100)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pbw54_Nw3TzL"
      },
      "source": [
        "# Finetune your LLM ü¶ô\n",
        "\n",
        "Finetuning has a simple interface. The basic premise is:\n",
        "1. Instantiate the LLM\n",
        "\n",
        "```\n",
        "    from llama import QuestionAnswerModel\n",
        "    model = QuestionAnswerModel()\n",
        "```\n",
        "2. Load your data into the LLM\n",
        "\n",
        "```\n",
        "    model.load_question_answer_from_jsonlines(\"seed_lamini_docs.jsonl\")\n",
        "\n",
        "```\n",
        "```\n",
        "    model.load_question_answer_from_csv(\"seed_bts.csv\")\n",
        "\n",
        "```\n",
        "3. Train the LLM\n",
        "\n",
        "```\n",
        "    model.train()\n",
        "\n",
        "```\n",
        "\n",
        "4. Compare your LLM: before and after training (optional)\n",
        "\n",
        "```\n",
        "    results = model.get_eval_results()\n",
        "\n",
        "```\n",
        "\n",
        "5. Run your trained LLM\n",
        "\n",
        "```\n",
        "    answer = model.get_answer(\"How can I add data to Lamini?\")\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9Nbqsj2OOnMr"
      },
      "outputs": [],
      "source": [
        "from llama import QuestionAnswerModel\n",
        "import time\n",
        "\n",
        "# Instantiate the model and load the data into it\n",
        "finetune_model = QuestionAnswerModel()\n",
        "finetune_model.load_question_answer_from_jsonlines(\"seed_lamini_docs.jsonl\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y1v3i-UOIY6d"
      },
      "source": [
        "# Model Support ü§ó\n",
        "To use different models for finetuning, you can pass in model_name parameter to QuestionAnswerModel(), for example:\n",
        "```\n",
        "  model = QuestionAnswerModel(model_name=\"YOUR_MODEL_NAME\")\n",
        "```\n",
        "Currently the free tier version supports limited models, you can find the list [here](https://lamini-ai.github.io/notebooks/#lamini-finetuning-for-free)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0YxBlP5Yrv1_",
        "outputId": "2e162ed7-79da-4e0e-eab1-fb7b413b5b27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training job submitted! Check status of job 2061 here: https://app.lamini.ai/train\n",
            "Finetuning process completed, model name is: 79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231\n",
            "Time taken: 309.2425570487976 seconds\n"
          ]
        }
      ],
      "source": [
        "# Train the model\n",
        "start=time.time()\n",
        "finetune_model.train()\n",
        "print(f\"Time taken: {time.time()-start} seconds\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# See the results üîÆ\n",
        "Once the model finishes training, you can view its responses, chat, and compare it to the base model on https://app.lamini.ai/train üëà\n",
        "\n",
        "More details on the finetune model vs base model are below üëá"
      ],
      "metadata": {
        "id": "toUr4imGQ2QI"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6qUD4s22rnK7",
        "outputId": "e2938f68-3fd6-47eb-f118-bb95aa253c1d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "----------------------------------------------------------------------------------------------------\n",
            "Training Results\n",
            "{'job_id': 2061, 'eval_results': [{'input': 'What can the `check_job_status()` function tell me about the progress of a task in Lamini? How do I use it to track the status of a job?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' The `check_job_status()` method in Lamini is a powerful tool for tracking progress and monitoring progress. It can be used to monitor the status of a job, and can also be used to update the status of the job as it progresses.'}]}, {'input': 'What kind of results can I expect to get from the `get_job_result()` function in Lamini? How can I use those results effectively?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' The `get_job_result()` method in Lamini is a powerful tool for generating text outputs for various tasks. It can be used to generate text outputs for various tasks, such as generating a response to a question, generating a response to a question with a specific answer, or generating a response to a question with an answer that is specific to the context in which the question is being asked.'}]}, {'input': 'How does the `cancel_job()` function help in Lamini? What does it mean to cancel a job, and when should I use this function?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' The `cancel_job()` function in Lamini is used to cancel a job. It is important to note that this function is only available when the `job_name` parameter is specified. If the `job_name` parameter does not specify a job, then the `cancel_job()` function will be called.'}]}, {'input': 'Can you explain the purpose of the `sample()` function in Lamini? How can I utilize it to generate meaningful outputs?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' The `sample()` function in Laminio is used to generate meaningful outputs. It is used to generate text that is meaningful to the user. It is not meant to be used for generating text that is not meaningful to the user.'}]}, {'input': \"Do I need any programming knowledge to use Lamini's functions effectively, or can I use them without coding experience?\", 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' Lamini is a powerful LLM engine that can be used to train models for a wide range of tasks. It is designed to be easy to use, and can be trained on any data available to it. It is important to note that the LLM Engine is not a replacement for programming skills, but rather a powerful tool that can be used to train models on a wide range of tasks.'}]}, {'input': 'How can I get started with Lamini if I have no technical background or programming skills?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' Lamini is a language model engine that can be used to generate text based on input data. It is not a programming language or a programming language model.'}]}, {'input': \"Is there any special setup or installation required to use Lamini's functions, or can I start using them right away?\", 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' Lamini is a powerful LLM engine that can be used for a wide range of applications. You can start using Lamini right away by installing it on your computer.'}]}, {'input': \"How can I obtain API keys to access Lamini's functionality? Are there any specific steps or requirements?\", 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' Lamini provides API keys to access its functionality. You can find them in the Lamini documentation.'}]}, {'input': \"Is there a cost associated with using Lamini's functions? Do I need to pay for the services or usage?\", 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': \" Lamini's functions are designed to be cost-effective and cost-effective is not the same as expensive. Lamini's functions are designed for use in a small business setting.\"}]}, {'input': 'Can I fine-tune the pre-trained models provided by Lamini on my own dataset? How does that process work?', 'outputs': [{'model_name': '79eeddaeda6522144c40efbf9157493672c68851d580ac2a1f7315d84cf35231', 'output': ' Yes, you can fine-tune the pre-loaded models provided by Lamini on your own dataset. Lamini provides a python library for this purpose. You can use the Lamini_train_data_path parameter to specify the dataset you wish to fine-tune on.'}, {'model_name': 'Base model (EleutherAI/pythia-410m-deduped)', 'output': '\\n\\nA:\\n\\nYou can use the same pre-trained models as you use for your own dataset.\\nYou can use the same pretrained models as you use for your dataset.\\n\\nA:\\n\\nYou should use the same pre-trained models for both datasets.\\n\\nA:\\n\\nI would recommend using the same pre-trained models for your own dataset.\\n\\nA:\\n  You can use the same pre-train models as you use for your own data.\\n\\nA:\\n\\nIf you use the same pre-trained models, you can use the same pre-trained model for both datasets.\\n\\nA :\\n\\nYou can use the same model for both datasets.\\n\\nYes, you can use the same model for both.\\n\\nA:\\n\\nYes, you can use both datasets.\\n\\nYes, but you can use the same model for your own dataset.\\n\\nQ:\\n\\nHow to get the value of a variable in a function?\\n\\nI have a function that takes a variable and returns a string.\\nfunction get_value(var_name) {\\n    var value = \"\";\\n    var value_str = \"\";\\n    var value_val = \"\";\\n'}]}]}\n",
            "----------------------------------------------------------------------------------------------------\n"
          ]
        }
      ],
      "source": [
        "# Evaluate base and finetuned models to compare performance\n",
        "results = finetune_model.get_eval_results()\n",
        "print_training_results(results)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vg1-xQBGidBT"
      },
      "source": [
        "## Congratulations, you've finetuned an LLM üéâ\n",
        "\n",
        "As you can see, the base model is really off the rails. Meanwhile, finetuning got the LLM to answer the question correctly and coherently!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KqAiNM-omysq"
      },
      "source": [
        "## Thanks for the tiny LLM, I'm ready for the real deal üí™\n",
        "If you want to build larger LLMs, run this live in production, host this on your own infrastructure (e.g. VPC or on premise), or other enterprise features, [please contact us](https://www.lamini.ai/contact)."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}